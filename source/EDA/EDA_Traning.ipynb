{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81955f92",
   "metadata": {},
   "source": [
    "# EDA for Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d117c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f514703",
   "metadata": {},
   "source": [
    "# Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b5ea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read config file - pay attention to correct path\n",
    "with open(\"../config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Corrected: should now be config[\"path\"][\"global_path\"]\n",
    "global_path = config[\"path\"][\"global_path\"]\n",
    "\n",
    "# Build data paths\n",
    "data_path = os.path.join(global_path, \"data\", \"raw\", \"jane-street-real-time-market-data-forecasting\")\n",
    "train_path = os.path.join(data_path, \"train.parquet\")\n",
    "\n",
    "# Method 1: directly specify partition path\n",
    "partition_0_path = os.path.join(train_path, \"partition_id=0\")\n",
    "train_partition_0 = pl.scan_parquet(partition_0_path)\n",
    "\n",
    "# View first 10 rows of data\n",
    "sample_data = train_partition_0.limit(10).collect()\n",
    "print(f\"\\nPartition 0 size of top 10 rows: {sample_data.shape}\")\n",
    "print(f\"rows: {len(sample_data)}\")\n",
    "\n",
    "# Display all column names vertically - clearer and more readable\n",
    "print(f\"\\nAll column names (total {len(sample_data.columns)} columns):\")\n",
    "for i, col in enumerate(sample_data.columns, 1):\n",
    "    print(f\"{i:3d}. {col}\")\n",
    "\n",
    "print(f\"\\nFirst 10 rows of data:\")\n",
    "print(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2d5c3e",
   "metadata": {},
   "source": [
    "# Trend of Reponders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd4487",
   "metadata": {},
   "source": [
    "47127338 rows, 1699 date_ids, over 4.5 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438b89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = []\n",
    "for i in range(9):\n",
    "    partition_path = os.path.join(train_path, f\"partition_id={i}\")\n",
    "    partitions.append(partition_path)\n",
    "\n",
    "responder_cols = [f'responder_{i}' for i in range(9)]\n",
    "needed_cols = ['date_id', 'time_id'] + responder_cols\n",
    "\n",
    "partitions_data = []\n",
    "for partition_id in partitions:\n",
    "    partition_path = os.path.join(train_path, f\"partition_id={partition_id}\") \n",
    "    partition_data = pl.scan_parquet(partition_path).select(needed_cols).collect()\n",
    "    partitions_data.append(partition_data)\n",
    "\n",
    "data = pl.concat(partitions_data)\n",
    "\n",
    "df = data.to_pandas()\n",
    "df.shape\n",
    "\n",
    "daily_means_full = df.groupby('date_id')[responder_cols].mean().reset_index()\n",
    "daily_means_full = daily_means_full.sort_values('date_id').reset_index(drop=True)\n",
    "\n",
    "print(f\"Date range: {daily_means_full['date_id'].min()} to {daily_means_full['date_id'].max()}\")\n",
    "\n",
    "# Calculate cumulative sums\n",
    "cumulative_data = daily_means_full.copy()\n",
    "for col in responder_cols:\n",
    "    cumulative_data[f'{col}_cumsum'] = daily_means_full[col].cumsum()\n",
    "\n",
    "# Create the enhanced plot with full data\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Define colors for each responder\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(responder_cols)))\n",
    "\n",
    "# Plot cumulative responder values\n",
    "for i, responder in enumerate(responder_cols):\n",
    "    cumsum_col = f'{responder}_cumsum'\n",
    "    plt.plot(range(len(cumulative_data)), cumulative_data[cumsum_col], \n",
    "             label=responder, color=colors[i], linewidth=2, alpha=0.8)\n",
    "\n",
    "plt.title(f'Cumulative resp and time horizons 1, 2, 3,4,5,6,7 and 8 ({len(cumulative_data)} days) - FULL DATASET', \n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Trade', fontsize=14)\n",
    "plt.ylabel('Cumulative Response', fontsize=14)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "y_min = min([cumulative_data[f'{col}_cumsum'].min() for col in responder_cols])\n",
    "y_max = max([cumulative_data[f'{col}_cumsum'].max() for col in responder_cols])\n",
    "y_range = y_max - y_min\n",
    "plt.ylim(y_min - y_range*0.1, y_max + y_range*0.1)\n",
    "\n",
    "plt.text(0.02, 0.98, f'Total samples: {len(df):,}', \n",
    "         transform=plt.gca().transAxes, fontsize=10, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12599bf0",
   "metadata": {},
   "source": [
    "# Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebe1f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistics\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"Total samples processed: {len(df):,}\")\n",
    "print(f\"Total unique days: {len(cumulative_data)}\")\n",
    "print(f\"Date range: {daily_means_full['date_id'].min()} to {daily_means_full['date_id'].max()}\")\n",
    "\n",
    "print(f\"\\nFinal cumulative values:\")\n",
    "for responder in responder_cols:\n",
    "    cumsum_col = f'{responder}_cumsum'\n",
    "    final_value = cumulative_data[cumsum_col].iloc[-1]\n",
    "    print(f\"{responder}: {final_value:.6f}\")\n",
    "\n",
    "print(f\"\\nDaily statistics for responders (full dataset):\")\n",
    "print(daily_means_full[responder_cols].describe())\n",
    "\n",
    "# Calculate and display some additional insights\n",
    "print(f\"\\nAdditional insights:\")\n",
    "print(f\"Best performing responder: {responder_cols[np.argmax([cumulative_data[f'{col}_cumsum'].iloc[-1] for col in responder_cols])]}\")\n",
    "print(f\"Worst performing responder: {responder_cols[np.argmin([cumulative_data[f'{col}_cumsum'].iloc[-1] for col in responder_cols])]}\")\n",
    "\n",
    "# Calculate volatility (standard deviation of daily changes)\n",
    "print(f\"\\nVolatility (std of daily means):\")\n",
    "for responder in responder_cols:\n",
    "    volatility = daily_means_full[responder].std()\n",
    "    print(f\"{responder}: {volatility:.6f}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
