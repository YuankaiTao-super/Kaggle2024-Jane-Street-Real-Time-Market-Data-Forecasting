{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb71b13",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-16T03:50:18.607014Z",
     "iopub.status.busy": "2024-12-16T03:50:18.606190Z",
     "iopub.status.idle": "2024-12-16T03:50:28.586277Z",
     "shell.execute_reply": "2024-12-16T03:50:28.585262Z"
    },
    "papermill": {
     "duration": 9.987623,
     "end_time": "2024-12-16T03:50:28.588327",
     "exception": false,
     "start_time": "2024-12-16T03:50:18.600704",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip -q install rtdl_num_embeddings delu rtdl_revisiting_models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec1e0eda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T03:50:28.596672Z",
     "iopub.status.busy": "2024-12-16T03:50:28.596320Z",
     "iopub.status.idle": "2024-12-16T03:50:33.973785Z",
     "shell.execute_reply": "2024-12-16T03:50:33.973038Z"
    },
    "papermill": {
     "duration": 5.383775,
     "end_time": "2024-12-16T03:50:33.975879",
     "exception": false,
     "start_time": "2024-12-16T03:50:28.592104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import rtdl_num_embeddings\n",
    "from rtdl_num_embeddings import compute_bins\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, ConcatDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import delu\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "\n",
    "from tanm_reference import Model, make_parameter_groups\n",
    "\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "\n",
    "import joblib\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edaa635a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T03:50:33.983834Z",
     "iopub.status.busy": "2024-12-16T03:50:33.983459Z",
     "iopub.status.idle": "2024-12-16T03:50:34.053838Z",
     "shell.execute_reply": "2024-12-16T03:50:34.052940Z"
    },
    "papermill": {
     "duration": 0.076277,
     "end_time": "2024-12-16T03:50:34.055738",
     "exception": false,
     "start_time": "2024-12-16T03:50:33.979461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "feature_train_list = [f\"feature_{idx:02d}\" for idx in range(79)] \n",
    "\n",
    "\n",
    "target_col = \"responder_6\"\n",
    "\n",
    "feature_train = feature_train_list \\\n",
    "                + [f\"responder_{idx}_lag_1\" for idx in range(9)] \n",
    "\n",
    "start_dt = 800\n",
    "end_dt = 1577\n",
    "\n",
    "feature_cat = [\"feature_09\", \"feature_10\", \"feature_11\"]\n",
    "feature_cont = [item for item in feature_train if item not in feature_cat]\n",
    "std_feature = [i for i in feature_train_list if i not in feature_cat] + [f\"responder_{idx}_lag_1\" for idx in range(9)]\n",
    "\n",
    "# batch_size = 2048\n",
    "batch_size = 8192\n",
    "num_epochs = 4\n",
    "\n",
    "\n",
    "data_stats = joblib.load(\"/kaggle/input/jane-street-data-preprocessing/data_stats.pkl\")\n",
    "means = data_stats['mean']\n",
    "stds = data_stats['std']\n",
    "\n",
    "def standardize(df, feature_cols, means, stds):\n",
    "    return df.with_columns([\n",
    "        ((pl.col(col) - means[col]) / stds[col]).alias(col) for col in feature_cols\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7367b010",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T03:50:34.069909Z",
     "iopub.status.busy": "2024-12-16T03:50:34.069284Z",
     "iopub.status.idle": "2024-12-16T03:50:34.112311Z",
     "shell.execute_reply": "2024-12-16T03:50:34.111503Z"
    },
    "papermill": {
     "duration": 0.048752,
     "end_time": "2024-12-16T03:50:34.113970",
     "exception": false,
     "start_time": "2024-12-16T03:50:34.065218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_original = pl.scan_parquet(\"/kaggle/input/jane-street-data-preprocessing/training.parquet\")\n",
    "valid_original = pl.scan_parquet(\"/kaggle/input/jane-street-data-preprocessing/validation.parquet\")\n",
    "all_original = pl.concat([train_original, valid_original])\n",
    "\n",
    "# def get_category_mapping(df, column):\n",
    "#     unique_values = df.select([column]).unique().collect().to_series()\n",
    "#     return {cat: idx for idx, cat in enumerate(unique_values)}\n",
    "\n",
    "# category_mappings = {col: get_category_mapping(all_original, col) for col in feature_cat + ['symbol_id']}\n",
    "\n",
    "category_mappings = {'feature_09': {2: 0, 4: 1, 9: 2, 11: 3, 12: 4, 14: 5, 15: 6, 25: 7, 26: 8, 30: 9, 34: 10, 42: 11, 44: 12, 46: 13, 49: 14, 50: 15, 57: 16, 64: 17, 68: 18, 70: 19, 81: 20, 82: 21},\n",
    " 'feature_10': {1: 0, 2: 1, 3: 2, 4: 3, 5: 4, 6: 5, 7: 6, 10: 7, 12: 8},\n",
    " 'feature_11': {9: 0, 11: 1, 13: 2, 16: 3, 24: 4, 25: 5, 34: 6, 40: 7, 48: 8, 50: 9, 59: 10, 62: 11, 63: 12, 66: 13,\n",
    "  76: 14, 150: 15, 158: 16, 159: 17, 171: 18, 195: 19, 214: 20, 230: 21, 261: 22, 297: 23, 336: 24, 376: 25, 388: 26, 410: 27, 522: 28, 534: 29, 539: 30},\n",
    " 'symbol_id': {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 12, 13: 13, 14: 14, 15: 15, 16: 16, 17: 17, 18: 18, 19: 19,\n",
    "  20: 20, 21: 21, 22: 22, 23: 23, 24: 24, 25: 25, 26: 26, 27: 27, 28: 28, 29: 29, 30: 30, 31: 31, 32: 32, 33: 33, 34: 34, 35: 35, 36: 36, 37: 37, 38: 38},\n",
    " 'time_id' : {i : i for i in range(968)}}\n",
    "\n",
    "\n",
    "def encode_column(df, column, mapping):\n",
    "    def encode_category(category):\n",
    "        return mapping.get(category, -1)  \n",
    "    \n",
    "    return df.with_columns(\n",
    "        pl.col(column).map_elements(encode_category, return_dtype=pl.Int16).alias(column)\n",
    "    )\n",
    "\n",
    "for col in feature_cat + ['symbol_id', 'time_id']:\n",
    "    train_original = encode_column(train_original, col, category_mappings[col])\n",
    "    valid_original = encode_column(valid_original, col, category_mappings[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bc4a480",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T03:50:34.122256Z",
     "iopub.status.busy": "2024-12-16T03:50:34.121998Z",
     "iopub.status.idle": "2024-12-16T03:50:34.129144Z",
     "shell.execute_reply": "2024-12-16T03:50:34.128290Z"
    },
    "papermill": {
     "duration": 0.013756,
     "end_time": "2024-12-16T03:50:34.130726",
     "exception": false,
     "start_time": "2024-12-16T03:50:34.116970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data1 = train_original \\\n",
    "             .filter((pl.col(\"date_id\") >= start_dt) & (pl.col(\"date_id\") <= end_dt)) \\\n",
    "             .select(feature_train + [target_col, 'weight', 'symbol_id', 'time_id'])\n",
    "\n",
    "train_data2 = valid_original \\\n",
    "             .filter(pl.col(\"date_id\") <= end_dt) \\\n",
    "             .select(feature_train + [target_col, 'weight', 'symbol_id', 'time_id'])\n",
    "\n",
    "train_data = pl.concat([train_data1, train_data2])\n",
    "valid_data = valid_original \\\n",
    "             .filter(pl.col(\"date_id\") > end_dt)\\\n",
    "             .sort(['date_id', 'time_id'])\\\n",
    "             .select(feature_train + [target_col, 'weight', 'symbol_id', 'time_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b676658a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T03:50:34.137507Z",
     "iopub.status.busy": "2024-12-16T03:50:34.137247Z",
     "iopub.status.idle": "2024-12-16T03:51:55.612474Z",
     "shell.execute_reply": "2024-12-16T03:51:55.611579Z"
    },
    "papermill": {
     "duration": 81.484649,
     "end_time": "2024-12-16T03:51:55.618301",
     "exception": false,
     "start_time": "2024-12-16T03:50:34.133652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.6 s, sys: 46 s, total: 1min 35s\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_data_tensor = torch.tensor(train_data.collect().to_numpy(), dtype=torch.float32)\n",
    "valid_data_tensor = torch.tensor(valid_data.collect().to_numpy(), dtype=torch.float32)\n",
    "\n",
    "train_ds = TensorDataset(train_data_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(valid_data_tensor)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=False)\n",
    "\n",
    "# valid_data2_tensor = torch.tensor(valid_data_2.collect().to_numpy(), dtype=torch.float32)\n",
    "# valid2_ds = TensorDataset(valid_data2_tensor)\n",
    "# valid2_dl = DataLoader(valid2_ds, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=False)\n",
    "\n",
    "all_data = False\n",
    "if all_data:\n",
    "    train_ds = ConcatDataset([train_ds, valid_ds])\n",
    "    train_dl = DataLoader(train_ds, batch_size=batch_size, num_workers=4, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e18696",
   "metadata": {
    "papermill": {
     "duration": 0.002785,
     "end_time": "2024-12-16T03:51:55.624286",
     "exception": false,
     "start_time": "2024-12-16T03:51:55.621501",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "360328b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T03:51:55.631542Z",
     "iopub.status.busy": "2024-12-16T03:51:55.631228Z",
     "iopub.status.idle": "2024-12-16T03:51:55.635145Z",
     "shell.execute_reply": "2024-12-16T03:51:55.634436Z"
    },
    "papermill": {
     "duration": 0.009533,
     "end_time": "2024-12-16T03:51:55.636878",
     "exception": false,
     "start_time": "2024-12-16T03:51:55.627345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_cont_features = 85\n",
    "# n_cont_features = 89\n",
    "n_cat_features = 5\n",
    "n_classes = None\n",
    "# cat_cardinalities = [83, 13, 540, 40]\n",
    "cat_cardinalities = [23, 10, 32, 40, 969]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873ac0b2",
   "metadata": {
    "papermill": {
     "duration": 0.003072,
     "end_time": "2024-12-16T03:51:55.642944",
     "exception": false,
     "start_time": "2024-12-16T03:51:55.639872",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TabM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aecd024",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T03:51:55.649920Z",
     "iopub.status.busy": "2024-12-16T03:51:55.649680Z",
     "iopub.status.idle": "2024-12-16T03:51:55.653935Z",
     "shell.execute_reply": "2024-12-16T03:51:55.653130Z"
    },
    "papermill": {
     "duration": 0.00957,
     "end_time": "2024-12-16T03:51:55.655497",
     "exception": false,
     "start_time": "2024-12-16T03:51:55.645927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogCoshLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogCoshLoss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.log(torch.cosh(y_pred - y_true))\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e21ea48d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T03:51:55.662734Z",
     "iopub.status.busy": "2024-12-16T03:51:55.662494Z",
     "iopub.status.idle": "2024-12-16T03:51:57.219926Z",
     "shell.execute_reply": "2024-12-16T03:51:57.218880Z"
    },
    "papermill": {
     "duration": 1.563223,
     "end_time": "2024-12-16T03:51:57.221920",
     "exception": false,
     "start_time": "2024-12-16T03:51:55.658697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 840 ms, sys: 226 ms, total: 1.07 s\n",
      "Wall time: 1.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# TabM\n",
    "arch_type = 'tabm'\n",
    "bins = None\n",
    " \n",
    "\n",
    "k = 32 # 集成输出的数量\n",
    "model = Model(\n",
    "    n_num_features=n_cont_features,\n",
    "    cat_cardinalities=cat_cardinalities,\n",
    "    n_classes=n_classes,\n",
    "    backbone={\n",
    "        'type': 'MLP',\n",
    "        'n_blocks': 3 ,\n",
    "        'd_block': 512,\n",
    "        'dropout': 0.25,\n",
    "    },\n",
    "    bins=bins,\n",
    "    # num_embeddings=(\n",
    "    #     None\n",
    "    #     if bins is None\n",
    "    #     else {\n",
    "    #         'type': 'PiecewiseLinearEmbeddings',\n",
    "    #         'd_embedding': 16,\n",
    "    #         'activation': True,\n",
    "    #         'version': 'B',\n",
    "    #     }\n",
    "    # ),\n",
    "    num_embeddings=(\n",
    "        None\n",
    "        # {\n",
    "        #     'type': 'PeriodicEmbeddings',\n",
    "        #     'd_embedding': 16,\n",
    "        #     'lite':True,\n",
    "        # }\n",
    "    ),\n",
    "    arch_type=arch_type,\n",
    "    k=k,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    # Instead of model.parameters(),\n",
    "    make_parameter_groups(model),\n",
    "    lr=1e-4,\n",
    "    weight_decay=5e-3 ,\n",
    ")\n",
    "\n",
    "# loss_fn = nn.MSELoss()\n",
    "class R2Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(R2Loss, self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        mse_loss = torch.sum((y_pred - y_true) ** 2)\n",
    "        var_y = torch.sum(y_true ** 2)\n",
    "        loss = mse_loss / (var_y + 1e-38)\n",
    "        return loss\n",
    "\n",
    "# loss_fn = nn.HuberLoss(delta=0.2)\n",
    "loss_fn = R2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4570d5d",
   "metadata": {
    "papermill": {
     "duration": 0.003094,
     "end_time": "2024-12-16T03:51:57.228536",
     "exception": false,
     "start_time": "2024-12-16T03:51:57.225442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffdb5a09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T03:51:57.235833Z",
     "iopub.status.busy": "2024-12-16T03:51:57.235450Z",
     "iopub.status.idle": "2024-12-16T03:51:57.239721Z",
     "shell.execute_reply": "2024-12-16T03:51:57.238931Z"
    },
    "papermill": {
     "duration": 0.009781,
     "end_time": "2024-12-16T03:51:57.241313",
     "exception": false,
     "start_time": "2024-12-16T03:51:57.231532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "timer = delu.tools.Timer()\n",
    "patience = 5\n",
    "early_stopping = delu.tools.EarlyStopping(patience, mode=\"max\")\n",
    "best = {\n",
    "    \"val\": -math.inf,\n",
    "    \"epoch\": -1,\n",
    "}\n",
    "timer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93d3d2d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T03:51:57.248979Z",
     "iopub.status.busy": "2024-12-16T03:51:57.248479Z",
     "iopub.status.idle": "2024-12-16T05:10:42.798485Z",
     "shell.execute_reply": "2024-12-16T05:10:42.797408Z"
    },
    "papermill": {
     "duration": 4725.58037,
     "end_time": "2024-12-16T05:10:42.824780",
     "exception": false,
     "start_time": "2024-12-16T03:51:57.244410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3336/3336 [18:42<00:00,  2.97it/s, epoch=1/4, loss=0.980201, lr=1.000e-04]\n",
      "100%|██████████| 549/549 [00:58<00:00,  9.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_r2 = 0.012803, val_loss_mean=0.994603, val_r2=0.007818, [time] 0:19:41.476321\n",
      "🌸 New best epoch! 🌸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3336/3336 [18:43<00:00,  2.97it/s, epoch=2/4, loss=0.987410, lr=1.000e-04]\n",
      "100%|██████████| 549/549 [00:57<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_r2 = 0.018285, val_loss_mean=0.993813, val_r2=0.008848, [time] 0:39:23.354268\n",
      "🌸 New best epoch! 🌸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3336/3336 [18:42<00:00,  2.97it/s, epoch=3/4, loss=0.988630, lr=1.000e-04]\n",
      "100%|██████████| 549/549 [00:57<00:00,  9.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_r2 = 0.020953, val_loss_mean=0.993536, val_r2=0.009463, [time] 0:59:04.417713\n",
      "🌸 New best epoch! 🌸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3336/3336 [18:42<00:00,  2.97it/s, epoch=4/4, loss=0.987309, lr=1.000e-04]\n",
      "100%|██████████| 549/549 [00:57<00:00,  9.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train_r2 = 0.023066, val_loss_mean=0.993572, val_r2=0.009890, [time] 1:18:45.498718\n",
      "🌸 New best epoch! 🌸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def r2_val(y_true, y_pred, sample_weight):\n",
    "    residuals = sample_weight * (y_true - y_pred) ** 2\n",
    "    weighted_residual_sum = np.sum(residuals)\n",
    "\n",
    "    # Calculate weighted sum of squared true values (denominator)\n",
    "    weighted_true_sum = np.sum(sample_weight * (y_true) ** 2)\n",
    "\n",
    "    # Calculate weighted R2\n",
    "    r2 = 1 - weighted_residual_sum / weighted_true_sum\n",
    "\n",
    "    return r2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    # Training\n",
    "    train_pred_list = []\n",
    "    with tqdm(train_dl, total=len(train_dl), leave=True) as phar:\n",
    "        for train_tensor in phar:\n",
    "            optimizer.zero_grad()\n",
    "            X_input = train_tensor[0][:, :-4].to(device)\n",
    "            y_input = train_tensor[0][:, -4].to(device)\n",
    "            w_input = train_tensor[0][:, -3].to(device)\n",
    "\n",
    "            \n",
    "            symbol_input = train_tensor[0][:, -2].to(device)\n",
    "            time_input = train_tensor[0][:, -1].to(device)\n",
    "                \n",
    "            x_cont_input = X_input[:, [col for col in range(X_input.shape[1]) if col not in [9, 10, 11]]]\n",
    "            x_cont_input = x_cont_input + torch.randn_like(x_cont_input) * 0.035\n",
    "            \n",
    "            x_cat_input = X_input[:, [9, 10, 11]]\n",
    "            x_cat_input = (torch.concat([x_cat_input, symbol_input.unsqueeze(-1), time_input.unsqueeze(-1)], axis=1)).to(torch.int64)\n",
    "\n",
    "            \n",
    "\n",
    "            output = model(x_cont_input, x_cat_input).squeeze(-1)\n",
    "            loss = loss_fn(output.flatten(0, 1), y_input.repeat_interleave(k))\n",
    "\n",
    "            train_pred_list.append((output.mean(1), y_input, w_input))\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            phar.set_postfix(\n",
    "                OrderedDict(\n",
    "                    epoch=f'{epoch+1}/{num_epochs}',\n",
    "                    loss=f'{loss.item():.6f}',\n",
    "                    lr=f'{optimizer.param_groups[0][\"lr\"]:.3e}'\n",
    "                )\n",
    "            )\n",
    "            phar.update(1)\n",
    "\n",
    "    weights_train = torch.cat([x[2] for x in train_pred_list]).cpu().numpy()\n",
    "    y_train = torch.cat([x[1] for x in train_pred_list]).cpu().numpy()\n",
    "    prob_train = torch.cat([x[0] for x in train_pred_list]).detach().cpu().numpy()\n",
    "    train_r2 = r2_val(y_train, prob_train, weights_train)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    valid_loss_list = []\n",
    "    valid_pred_list = []\n",
    "    for valid_tensor in tqdm(valid_dl):\n",
    "        X_valid = valid_tensor[0][:, :-4].to(device)\n",
    "        y_valid = valid_tensor[0][:, -4].to(device)\n",
    "        w_valid = valid_tensor[0][:, -3].to(device)\n",
    "        symbol_valid = valid_tensor[0][:, -2].to(device)\n",
    "        time_valid = valid_tensor[0][:, -1].to(device)\n",
    "        \n",
    "        x_cont_valid = X_valid[:, [col for col in range(X_valid.shape[1]) if col not in [9, 10, 11]]]\n",
    "        x_cont_valid = x_cont_valid + torch.randn_like(x_cont_valid) * 0.035\n",
    "        \n",
    "        x_cat_valid = X_valid[:, [9, 10, 11]]\n",
    "        x_cat_valid = (torch.concat([x_cat_valid, symbol_valid.unsqueeze(-1),time_valid.unsqueeze(-1)], axis=1)).to(torch.int64)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x_cont_valid, x_cat_valid).squeeze(-1)\n",
    "    \n",
    "        # val_loss = loss_fn(y_pred.squeeze(-1).squeeze(-1).cpu().detach(), y_valid)\n",
    "        val_loss = loss_fn(y_pred.flatten(0, 1), y_valid.repeat_interleave(k))\n",
    "        valid_loss_list.append(val_loss)\n",
    "        valid_pred_list.append((y_pred.mean(1), y_valid, w_valid))\n",
    "    \n",
    "    valid_loss_mean = sum(valid_loss_list) / len(valid_loss_list)\n",
    "    # val_r2 = r2_score(y_valid_data, torch.cat(valid_pred_list).numpy(), sample_weight=w_valid_data)\n",
    "\n",
    "    weights_eval = torch.cat([x[2] for x in valid_pred_list]).cpu().numpy()\n",
    "    y_eval = torch.cat([x[1] for x in valid_pred_list]).cpu().numpy()\n",
    "    prob_eval = torch.cat([x[0] for x in valid_pred_list]).cpu().numpy()\n",
    "    val_r2 = r2_val(y_eval, prob_eval, weights_eval)\n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}: train_r2 = {train_r2:.6f}, val_loss_mean={valid_loss_mean:.6f}, val_r2={val_r2:.6f}, [time] {timer}\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if val_r2 > best[\"val\"]:\n",
    "        print(\"🌸 New best epoch! 🌸\")\n",
    "        best = {\"val\": val_r2, \"epoch\": epoch}\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'r2': val_r2,\n",
    "        }\n",
    "        torch.save(checkpoint, f'epoch{epoch}_r2_{val_r2}.pt')\n",
    "    print()\n",
    "    \n",
    "    early_stopping.update(val_r2)\n",
    "    if early_stopping.should_stop():\n",
    "        print(\"Early stop\")\n",
    "        break\n",
    "\n",
    "\n",
    "checkpoint = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    # 'r2': val_r2,\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, f'last_tabm.pt')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    },
    {
     "sourceId": 207787842,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 212973694,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4831.932203,
   "end_time": "2024-12-16T05:10:48.065100",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-16T03:50:16.132897",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
